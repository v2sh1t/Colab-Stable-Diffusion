{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown <center><h1>Install</h1></center>\n",
        "\n",
        "%cd /content\n",
        "!git clone -b totoro6 https://github.com/LucipherDev/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.28.post2\n",
        "!apt -y install -qq aria2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <center><h1>Load Models</h1></center>\n",
        "\n",
        "import torch\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "\n",
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "\n",
        "flux_version = \"dev\" # @param [\"dev\",\"schnell\"]\n",
        "\n",
        "print(f\"Downloading Flux.1-{flux_version}...\")\n",
        "\n",
        "if flux_version == \"schnell\":\n",
        "  !aria2c --quiet --console-log-level=error --auto-file-renaming=false --allow-overwrite=false -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors -d /content/TotoroUI/models/unet -o flux1-schnell.safetensors\n",
        "elif flux_version == \"dev\":\n",
        "  !aria2c --quiet --console-log-level=error --auto-file-renaming=false --allow-overwrite=false -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors -d /content/TotoroUI/models/unet -o flux1-dev.safetensors\n",
        "\n",
        "print(\"Downloading VAE...\")\n",
        "!aria2c --quiet --console-log-level=error --auto-file-renaming=false --allow-overwrite=false -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/TotoroUI/models/vae -o ae.sft\n",
        "\n",
        "print(\"Downloading Clips...\")\n",
        "!aria2c --quiet --console-log-level=error --auto-file-renaming=false --allow-overwrite=false -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/TotoroUI/models/clip -o clip_l.safetensors\n",
        "!aria2c --quiet --console-log-level=error --auto-file-renaming=false --allow-overwrite=false -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d /content/TotoroUI/models/clip -o t5xxl_fp8_e4m3fn.safetensors\n",
        "\n",
        "with torch.inference_mode():\n",
        "    print(\"Loading VAE...\")\n",
        "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "    print(f\"Loading Flux.1-{flux_version}...\")\n",
        "    unet = UNETLoader.load_unet(f\"flux1-{flux_version}.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "    print(\"Loading Clips...\")\n",
        "    clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "\n",
        "    unet_f, clip_f = unet, clip\n",
        "\n",
        "print(\"All Models Loaded!\")"
      ],
      "metadata": {
        "id": "k3aTOrdb8HxC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown <center><h1>Functions</h1></center>\n",
        "\n",
        "import re\n",
        "import os\n",
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "import nodes\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro_extras import nodes_post_processing\n",
        "from totoro_extras import nodes_flux\n",
        "from totoro import model_management\n",
        "\n",
        "CLIPTextEncodeFlux = nodes_flux.NODE_CLASS_MAPPINGS[\"CLIPTextEncodeFlux\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "LoraLoader = NODE_CLASS_MAPPINGS[\"LoraLoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "VAEEncode = NODE_CLASS_MAPPINGS[\"VAEEncode\"]()\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
        "ImageScaleToTotalPixels = nodes_post_processing.NODE_CLASS_MAPPINGS[\"ImageScaleToTotalPixels\"]()\n",
        "\n",
        "\n",
        "loras = {\n",
        "    \"xlabs_flux_anime\":\n",
        "     {\n",
        "         \"url\": \"https://huggingface.co/XLabs-AI/flux-lora-collection/resolve/main/anime_lora_comfy_converted.safetensors\",\n",
        "         \"filename\": \"xlabs_anime_lora.safetensors\",\n",
        "         \"triggers\": \"anime\"\n",
        "         },\n",
        "    \"xlabs_flux_art\":\n",
        "     {\n",
        "         \"url\": \"https://huggingface.co/XLabs-AI/flux-lora-collection/resolve/main/art_lora_comfy_converted.safetensors\",\n",
        "         \"filename\": \"xlabs_art_lora.safetensors\",\n",
        "         \"triggers\": \"art\"\n",
        "         },\n",
        "    \"xlabs_flux_disney\":\n",
        "     {\n",
        "         \"url\": \"https://huggingface.co/XLabs-AI/flux-lora-collection/resolve/main/disney_lora_comfy_converted.safetensors\",\n",
        "         \"filename\": \"xlabs_disney_lora.safetensors\",\n",
        "         \"triggers\": \"disney style\"\n",
        "         },\n",
        "    \"xlabs_flux_mjv6\":\n",
        "     {\n",
        "         \"url\": \"https://huggingface.co/XLabs-AI/flux-lora-collection/resolve/main/mjv6_lora_comfy_converted.safetensors\",\n",
        "         \"filename\": \"xlabs_mjv6_lora.safetensors\",\n",
        "         },\n",
        "    \"xlabs_flux_realism\":\n",
        "     {\n",
        "         \"url\": \"https://huggingface.co/XLabs-AI/flux-lora-collection/resolve/main/realism_lora_comfy_converted.safetensors\",\n",
        "         \"filename\": \"xlabs_realism_lora.safetensors\",\n",
        "         },\n",
        "    \"xlabs_flux_scenery\":\n",
        "     {\n",
        "         \"url\": \"https://huggingface.co/XLabs-AI/flux-lora-collection/resolve/main/scenery_lora_comfy_converted.safetensors\",\n",
        "         \"filename\": \"xlabs_scenery_lora.safetensors\",\n",
        "         \"triggers\": \"scenery style\"\n",
        "         },\n",
        "    # \"xlabs_flux_furry\":\n",
        "    #  {\n",
        "    #      \"url\": \"https://huggingface.co/XLabs-AI/flux-lora-collection/resolve/main/furry_lora.safetensors\",\n",
        "    #     \"filename\": \"xlabs_flux_furry_lora.safetensors\",\n",
        "    #     },\n",
        "}\n",
        "\n",
        "def load_loras(prompt):\n",
        "  # @markdown <ul><li><h2>Load Loras</h2>- Add &lt;lora_name:model_strength&gt; to Prompt</li></ul>\n",
        "\n",
        "  global unet, clip, unet_f, clip_f\n",
        "\n",
        "  unet_f, clip_f = unet, clip\n",
        "\n",
        "  matches = re.findall(r\"<\\s*([^:]+?)\\s*:\\s*([0-9.]+)\\s*>\", prompt)\n",
        "\n",
        "  loras_list = [(name.strip(), float(value)) for name, value in matches]\n",
        "\n",
        "  if len(loras_list):\n",
        "    print(\"Loading Loras...\")\n",
        "\n",
        "  for lora_tuple in loras_list:\n",
        "    lora = loras.get(lora_tuple[0], None)\n",
        "\n",
        "    if lora:\n",
        "      !aria2c --quiet --console-log-level=error --auto-file-renaming=false --allow-overwrite=false -c -x 16 -s 16 -k 1M {lora[\"url\"]} -d /content/TotoroUI/models/loras -o {lora[\"filename\"]}\n",
        "\n",
        "      with torch.inference_mode():\n",
        "        unet_f, clip_f = LoraLoader.load_lora(unet_f, clip_f, lora[\"filename\"], lora_tuple[1], lora_tuple[1])\n",
        "\n",
        "      print(f\"Loaded Lora: {lora_tuple[0]}\")\n",
        "    else:\n",
        "      print(f\"Lora not listed: {lora_tuple[0]}\")\n",
        "\n",
        "def clean_prompt(prompt):\n",
        "  cleaned_prompt = re.sub(r\"<.*?>\", \"\", prompt)\n",
        "\n",
        "  return cleaned_prompt\n",
        "\n",
        "def cuda_gc():\n",
        "  try:\n",
        "    model_management.soft_empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2\n",
        "\n",
        "def save_image(decoded, path, name, download=False):\n",
        "  full_path = os.path.abspath(os.path.join(path, name))\n",
        "  Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save( full_path)\n",
        "\n",
        "  img = Image.open(full_path)\n",
        "  display(img)\n",
        "\n",
        "  if download:\n",
        "    files.download(full_path)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(prompt, width, height, fixed_seed, guidance, steps, sampler_name, scheduler, batch_size, auto_download, mode=\"t2i\", input_img=None, denoise=1.0):\n",
        "  global unet, clip, unet_f, clip_f\n",
        "\n",
        "  print(\"Prompt Received\")\n",
        "\n",
        "  load_loras(prompt)\n",
        "  prompt = clean_prompt(prompt)\n",
        "\n",
        "  if mode == \"t2i\":\n",
        "    latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
        "\n",
        "  elif mode == \"i2i\":\n",
        "    image = nodes.LoadImage().load_image(input_img)[0]\n",
        "    latent_image = ImageScaleToTotalPixels.upscale(image, \"lanczos\", 1.0)[0]\n",
        "    latent_image = VAEEncode.encode(vae, latent_image)[0]\n",
        "\n",
        "  cond = CLIPTextEncodeFlux.encode(clip_f, prompt, prompt, guidance)[0]\n",
        "  guider = BasicGuider.get_guider(unet_f, cond)[0]\n",
        "  sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "  sigmas = BasicScheduler.get_sigmas(unet_f, scheduler, steps, denoise)[0]\n",
        "\n",
        "  for i in range(0, batch_size):\n",
        "    if fixed_seed == 0:\n",
        "      seed = random.randint(0, 18446744073709551615)\n",
        "    else:\n",
        "      seed = fixed_seed\n",
        "\n",
        "    print(\"Seed:\", seed)\n",
        "\n",
        "    noise = RandomNoise.get_noise(seed)[0]\n",
        "    sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "    model_management.soft_empty_cache()\n",
        "    decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "\n",
        "    save_image(decoded, \"/content\", f\"flux_{mode}_{seed}_{i}.png\", auto_download)\n",
        "\n",
        "  cuda_gc()\n",
        "\n",
        "print(f\"{'Lora Name':<40} {'Trigger Words':<40}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for key_name, details in loras.items():\n",
        "    trigger_words = details.get(\"triggers\", \"N/A\")\n",
        "    print(f\"{key_name:<40} {trigger_words:<40}\")"
      ],
      "metadata": {
        "id": "mLGPKWvopwnC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ur9TmMNwC2kR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@markdown <center><h1>Txt2Img</h1></center>\n",
        "\n",
        "positive_prompt = \"black forest toast spelling out the words 'FLUX DEV', tasty, food photography, dynamic shot\" # @param {\"type\":\"string\"}\n",
        "width = 1024 # @param {\"type\":\"slider\",\"min\":256,\"max\":2048,\"step\":1}\n",
        "height = 1024 # @param {\"type\":\"slider\",\"min\":256,\"max\":2048,\"step\":1}\n",
        "fixed_seed = 0 # @param {\"type\":\"slider\",\"min\":0,\"max\":18446744073709552000,\"step\":1}\n",
        "guidance = 3.5 # @param {\"type\":\"slider\",\"min\":0,\"max\":20,\"step\":0.5}\n",
        "steps = 20 # @param {\"type\":\"slider\",\"min\":4,\"max\":50,\"step\":1}\n",
        "sampler_name = \"euler\" # @param [\"euler\",\"heun\",\"heunpp2\",\"heunpp2\",\"dpm_2\",\"lms\",\"dpmpp_2m\",\"ipndm\",\"deis\",\"ddim\",\"uni_pc\",\"uni_pc_bh2\"]\n",
        "scheduler = \"simple\" # @param [\"normal\",\"sgm_uniform\",\"simple\",\"ddim_uniform\"]\n",
        "batch_size = 1 # @param {\"type\":\"slider\",\"min\":1,\"max\":20,\"step\":1}\n",
        "auto_download = False # @param {\"type\":\"boolean\"}\n",
        "\n",
        "generate(positive_prompt, width, height, fixed_seed, guidance, steps, sampler_name, scheduler, batch_size, auto_download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Dpd2sfrePYoA"
      },
      "outputs": [],
      "source": [
        "#@markdown <center><h1>Img2Img</h1></center>\n",
        "\n",
        "positive_prompt = \"anime style\" # @param {\"type\":\"string\"}\n",
        "fixed_seed = 0 # @param {\"type\":\"slider\",\"min\":0,\"max\":18446744073709552000,\"step\":1}\n",
        "guidance = 3.5 # @param {\"type\":\"slider\",\"min\":0,\"max\":20,\"step\":0.5}\n",
        "steps = 20 # @param {\"type\":\"slider\",\"min\":4,\"max\":50,\"step\":1}\n",
        "sampler_name = \"euler\" # @param [\"euler\",\"heun\",\"heunpp2\",\"heunpp2\",\"dpm_2\",\"lms\",\"dpmpp_2m\",\"ipndm\",\"deis\",\"ddim\",\"uni_pc\",\"uni_pc_bh2\"]\n",
        "scheduler = \"simple\" # @param [\"normal\",\"sgm_uniform\",\"simple\",\"ddim_uniform\"]\n",
        "input_img = \"/content/test.png\" # @param {\"type\":\"string\"}\n",
        "denoise = 0.85 # @param {\"type\":\"slider\",\"min\":0,\"max\":1,\"step\":0.01}\n",
        "batch_size = 1 # @param {\"type\":\"slider\",\"min\":1,\"max\":20,\"step\":1}\n",
        "auto_download = False # @param {\"type\":\"boolean\"}\n",
        "\n",
        "\n",
        "generate(positive_prompt, 0, 0, fixed_seed, guidance, steps, sampler_name, scheduler, batch_size, auto_download, \"i2i\", input_img, denoise)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}